# -*- coding: utf-8 -*-
"""TA7_Gensim_Word2Vec_KeyedVectors_IvanFalconMonzon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M9rIN34j9dzDmotxVwOcn_70Mrl_KQeu

# *IVÁN FALCÓN MONZÓN*

Video tutorial de la tarea: https://youtu.be/Z1VsHYcNXDI?si=PxIOcNjWb1HItQqB

**INSTALAR GENSIM**
"""

!pip install gensim

"""📌 ¿Qué es Gensim?

Gensim es una biblioteca de Python especializada en el procesamiento de texto, aprendizaje automático y modelado de temas. Se utiliza principalmente en tareas de procesamiento de lenguaje natural (NLP) y análisis semántico.

Es especialmente eficiente para trabajar con grandes volúmenes de texto, ya que está diseñada para manejar datos de forma escalable, sin necesidad de cargar todo en memoria.

# **ENTRENAR WORD2VEC MODEL (1º PARTE)**

Descargar csv

https://www.kaggle.com/datasets/rootuser/worldnews-on-reddit
"""

# Importación de librerías necesarias
# Gensim: Librería para modelos de procesamiento de texto
from gensim.models import Word2Vec, keyedvectors

# Pandas: Librería para manipulación de datos en estructuras tipo DataFrame
import pandas as pd

# NLTK: Biblioteca para procesamiento de lenguaje natural
import nltk

# Se importa una librería diferente a la del video
# TreebankWordTokenizer: Tokenizador de NLTK basado en el estándar Penn Treebank
from nltk.tokenize import TreebankWordTokenizer

# Carga de datos desde un archivo CSV en un DataFrame de Pandas
df = pd.read_csv('reddit_worldnews_start_to_2016-11-22.csv')

# Muestra las primeras 10 filas del DataFrame
df.head(10)

# Extrae los títulos de noticias del DataFrame y los almacena en una variable
newsTiles = df['title'].values

# Mostrar el contenido de la variable newsTiles
newsTiles

# Inicializa el tokenizador de NLTK basado en el estándar Penn Treebank
tokenizer = TreebankWordTokenizer()

# Tokeniza cada título de noticia en newsTiles usando el tokenizador
newsVec = [tokenizer.tokenize(title) for title in newsTiles]

# Muestra los títulos tokenizados
newsVec

# Entrenamiento del modelo Word2Vec con los títulos tokenizados
model = Word2Vec(newsVec, min_count=1, vector_size=32)

# Encuentra las palabras más similares a 'man' en el espacio vectorial de Word2Vec
model.wv.most_similar('man')

# Realiza un cálculo de vectores para encontrar la relación entre 'King', 'man' y 'woman'
vec = model.wv['King'] - model.wv['man'] + model.wv['woman']

# Encuentra las palabras más similares al nuevo vector calculado
model.wv.most_similar([vec])

# Muestra el vector correspondiente a la palabra 'man' en el modelo Word2Vec
model.wv['man']

"""# **USAR PRE-ENTRENAMIENTO WORD2VEC MODEL, KEYEDVECTORS (2º PARTE)**

Descargar Pretrain Word2vec model

https://drive.usercontent.google.com/download?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download&authuser=1
"""

# Importación de la clase KeyedVectors de gensim para manejar modelos preentrenados
from gensim.models import KeyedVectors  # Importación correcta

# Importación de la librería para interactuar con Google Drive en Google Colab
from google.colab import drive

# Montar Google Drive para acceder a los archivos almacenados en él
drive.mount('/content/drive')

# Especificar la ruta completa del archivo del modelo de Word2Vec (se debe cambiar la ruta según dónde se tenga el archivo)
file_path = '/content/drive/MyDrive/7PRO/GoogleNews-vectors-negative300.bin'

# Cargar el modelo preentrenado de Word2Vec desde el archivo binario usando la ruta especificada
# El parámetro 'binary=True' indica que el archivo está en formato binario y 'limit=100000' carga solo las primeras 100,000 palabras.
model = KeyedVectors.load_word2vec_format(file_path, binary=True, limit=100000)

# Confirmar que el modelo se ha cargado correctamente
print("Modelo cargado correctamente.")

# Realiza una operación vectorial para obtener una relación semántica entre 'King', 'man' y 'woman'
vec = model['King'] - model['man'] + model['woman']

# Encuentra las palabras más similares al vector calculado
model.most_similar([vec])

# Realiza una operación vectorial para obtener una relación semántica entre 'Germany', 'Berlin' y 'Madrid'
vec = model['Germany'] - model['Berlin'] + model['Madrid']

# Encuentra las palabras más similares al vector calculado
model.most_similar([vec])

# Realiza una operación vectorial para obtener una relación semántica entre 'Messi', 'football' y 'cricket'
vec = model['Messi'] - model['football'] + model['cricket']

# Encuentra las palabras más similares al vector calculado
model.most_similar([vec])

# Realiza una operación vectorial para obtener una relación semántica entre 'Messi', 'football' y 'tennis'
vec = model['Messi'] - model['football'] + model['tennis']

# Encuentra las palabras más similares al vector calculado
model.most_similar([vec])

"""# **Repositorios:**

Google Colab: https://colab.research.google.com/drive/1M9rIN34j9dzDmotxVwOcn_70Mrl_KQeu?usp=sharing

 Github: https://github.com/IvanFalconMonzon/TA7_Gensim_Word2Vec_KeyedVectors_IvanFalconMonzon.git
"""