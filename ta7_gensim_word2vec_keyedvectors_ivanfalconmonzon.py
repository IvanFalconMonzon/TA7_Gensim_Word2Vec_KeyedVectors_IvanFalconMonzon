# -*- coding: utf-8 -*-
"""TA7_Gensim_Word2Vec_KeyedVectors_IvanFalconMonzon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M9rIN34j9dzDmotxVwOcn_70Mrl_KQeu

# *IV츼N FALC칍N MONZ칍N*

Video tutorial de la tarea: https://youtu.be/Z1VsHYcNXDI?si=PxIOcNjWb1HItQqB

**INSTALAR GENSIM**
"""

!pip install gensim

"""游늷 쯈u칠 es Gensim?

Gensim es una biblioteca de Python especializada en el procesamiento de texto, aprendizaje autom치tico y modelado de temas. Se utiliza principalmente en tareas de procesamiento de lenguaje natural (NLP) y an치lisis sem치ntico.

Es especialmente eficiente para trabajar con grandes vol칰menes de texto, ya que est치 dise침ada para manejar datos de forma escalable, sin necesidad de cargar todo en memoria.

# **ENTRENAR WORD2VEC MODEL (1췈 PARTE)**

Descargar csv

https://www.kaggle.com/datasets/rootuser/worldnews-on-reddit
"""

# Importaci칩n de librer칤as necesarias
# Gensim: Librer칤a para modelos de procesamiento de texto
from gensim.models import Word2Vec, keyedvectors

# Pandas: Librer칤a para manipulaci칩n de datos en estructuras tipo DataFrame
import pandas as pd

# NLTK: Biblioteca para procesamiento de lenguaje natural
import nltk

# Se importa una librer칤a diferente a la del video
# TreebankWordTokenizer: Tokenizador de NLTK basado en el est치ndar Penn Treebank
from nltk.tokenize import TreebankWordTokenizer

# Carga de datos desde un archivo CSV en un DataFrame de Pandas
df = pd.read_csv('reddit_worldnews_start_to_2016-11-22.csv')

# Muestra las primeras 10 filas del DataFrame
df.head(10)

# Extrae los t칤tulos de noticias del DataFrame y los almacena en una variable
newsTiles = df['title'].values

# Mostrar el contenido de la variable newsTiles
newsTiles

# Inicializa el tokenizador de NLTK basado en el est치ndar Penn Treebank
tokenizer = TreebankWordTokenizer()

# Tokeniza cada t칤tulo de noticia en newsTiles usando el tokenizador
newsVec = [tokenizer.tokenize(title) for title in newsTiles]

# Muestra los t칤tulos tokenizados
newsVec

# Entrenamiento del modelo Word2Vec con los t칤tulos tokenizados
model = Word2Vec(newsVec, min_count=1, vector_size=32)

# Encuentra las palabras m치s similares a 'man' en el espacio vectorial de Word2Vec
model.wv.most_similar('man')

# Realiza un c치lculo de vectores para encontrar la relaci칩n entre 'King', 'man' y 'woman'
vec = model.wv['King'] - model.wv['man'] + model.wv['woman']

# Encuentra las palabras m치s similares al nuevo vector calculado
model.wv.most_similar([vec])

# Muestra el vector correspondiente a la palabra 'man' en el modelo Word2Vec
model.wv['man']

"""# **USAR PRE-ENTRENAMIENTO WORD2VEC MODEL, KEYEDVECTORS (2췈 PARTE)**

Descargar Pretrain Word2vec model

https://drive.usercontent.google.com/download?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download&authuser=1
"""

# Importaci칩n de la clase KeyedVectors de gensim para manejar modelos preentrenados
from gensim.models import KeyedVectors  # Importaci칩n correcta

# Importaci칩n de la librer칤a para interactuar con Google Drive en Google Colab
from google.colab import drive

# Montar Google Drive para acceder a los archivos almacenados en 칠l
drive.mount('/content/drive')

# Especificar la ruta completa del archivo del modelo de Word2Vec (se debe cambiar la ruta seg칰n d칩nde se tenga el archivo)
file_path = '/content/drive/MyDrive/7PRO/GoogleNews-vectors-negative300.bin'

# Cargar el modelo preentrenado de Word2Vec desde el archivo binario usando la ruta especificada
# El par치metro 'binary=True' indica que el archivo est치 en formato binario y 'limit=100000' carga solo las primeras 100,000 palabras.
model = KeyedVectors.load_word2vec_format(file_path, binary=True, limit=100000)

# Confirmar que el modelo se ha cargado correctamente
print("Modelo cargado correctamente.")

# Realiza una operaci칩n vectorial para obtener una relaci칩n sem치ntica entre 'King', 'man' y 'woman'
vec = model['King'] - model['man'] + model['woman']

# Encuentra las palabras m치s similares al vector calculado
model.most_similar([vec])

# Realiza una operaci칩n vectorial para obtener una relaci칩n sem치ntica entre 'Germany', 'Berlin' y 'Madrid'
vec = model['Germany'] - model['Berlin'] + model['Madrid']

# Encuentra las palabras m치s similares al vector calculado
model.most_similar([vec])

# Realiza una operaci칩n vectorial para obtener una relaci칩n sem치ntica entre 'Messi', 'football' y 'cricket'
vec = model['Messi'] - model['football'] + model['cricket']

# Encuentra las palabras m치s similares al vector calculado
model.most_similar([vec])

# Realiza una operaci칩n vectorial para obtener una relaci칩n sem치ntica entre 'Messi', 'football' y 'tennis'
vec = model['Messi'] - model['football'] + model['tennis']

# Encuentra las palabras m치s similares al vector calculado
model.most_similar([vec])

"""# **Repositorios:**

Google Colab: https://colab.research.google.com/drive/1M9rIN34j9dzDmotxVwOcn_70Mrl_KQeu?usp=sharing

 Github: https://github.com/IvanFalconMonzon/TA7_Gensim_Word2Vec_KeyedVectors_IvanFalconMonzon.git
"""